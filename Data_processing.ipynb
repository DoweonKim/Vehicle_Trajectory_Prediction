{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43236206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc6fbb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cleaned dataset...\n",
      "Dataset shape: (33536, 17)\n",
      "Columns: ['Vehicle_ID', 'Frame_ID', 'Global_Time', 'Local_X', 'Local_Y', 'v_Vel', 'v_Acc', 'Lane_ID', 'Preceding', 'Following', 'delta_x', 'delta_y', 'delta_t', 'speed_calc', 'accel_calc', 'heading', 'lane_change']\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 1. Load cleaned dataset\n",
    "# ============================================\n",
    "print(\"Loading cleaned dataset...\")\n",
    "df = pd.read_csv(\"cleaned_us101_final_clean.csv\")\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "638dd17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit conversion complete (ft → m).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 2. Convert units (ft → meter)\n",
    "# ============================================\n",
    "feet_to_meter = 0.3048\n",
    "convert_cols = [\"Local_X\", \"Local_Y\", \"v_Vel\", \"v_Acc\",\n",
    "                \"speed_calc\", \"accel_calc\"]\n",
    "\n",
    "for col in convert_cols:\n",
    "    df[col] = df[col] * feet_to_meter\n",
    "\n",
    "print(\"Unit conversion complete (ft → m).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07731ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values:\n",
      "Vehicle_ID     0\n",
      "Frame_ID       0\n",
      "Global_Time    0\n",
      "Local_X        0\n",
      "Local_Y        0\n",
      "v_Vel          0\n",
      "v_Acc          0\n",
      "Lane_ID        0\n",
      "Preceding      0\n",
      "Following      0\n",
      "delta_x        0\n",
      "delta_y        0\n",
      "delta_t        0\n",
      "speed_calc     0\n",
      "accel_calc     0\n",
      "heading        0\n",
      "lane_change    0\n",
      "dtype: int64\n",
      "\n",
      "Unique Vehicle_ID count: 488\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3. Check for missing values\n",
    "# ============================================\n",
    "print(\"\\nMissing values:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "print(\"\\nUnique Vehicle_ID count:\", df[\"Vehicle_ID\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba3fefd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature columns: ['Local_X', 'Local_Y', 'v_Vel', 'v_Acc', 'speed_calc', 'accel_calc']\n",
      "Target columns: ['Local_X', 'Local_Y']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================\n",
    "# 4. Define feature and target columns\n",
    "# ============================================\n",
    "feature_cols = [\"Local_X\", \"Local_Y\", \"v_Vel\",\n",
    "                \"v_Acc\", \"speed_calc\", \"accel_calc\"]\n",
    "\n",
    "target_cols = [\"Local_X\", \"Local_Y\"]   # prediction target\n",
    "\n",
    "print(\"\\nFeature columns:\", feature_cols)\n",
    "print(\"Target columns:\", target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81ccd837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting dataset by Vehicle_ID...\n",
      "Train size: (23449, 17) Vehicles: 341\n",
      "Val size: (5019, 17) Vehicles: 73\n",
      "Test size: (5068, 17) Vehicles: 74\n",
      "Vehicle ID split verified. No leakage.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 5. Train/Val/Test split by Vehicle_ID (no leakage)\n",
    "# ============================================\n",
    "print(\"\\nSplitting dataset by Vehicle_ID...\")\n",
    "\n",
    "vehicle_ids = df[\"Vehicle_ID\"].unique()\n",
    "\n",
    "train_ids, temp_ids = train_test_split(vehicle_ids, test_size=0.30, random_state=42)\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=0.50, random_state=42)\n",
    "\n",
    "train_df = df[df[\"Vehicle_ID\"].isin(train_ids)].reset_index(drop=True)\n",
    "val_df   = df[df[\"Vehicle_ID\"].isin(val_ids)].reset_index(drop=True)\n",
    "test_df  = df[df[\"Vehicle_ID\"].isin(test_ids)].reset_index(drop=True)\n",
    "\n",
    "print(\"Train size:\", train_df.shape, \"Vehicles:\", train_df[\"Vehicle_ID\"].nunique())\n",
    "print(\"Val size:\", val_df.shape,   \"Vehicles:\", val_df[\"Vehicle_ID\"].nunique())\n",
    "print(\"Test size:\", test_df.shape, \"Vehicles:\", test_df[\"Vehicle_ID\"].nunique())\n",
    "\n",
    "# No overlap check\n",
    "assert len(set(train_ids) & set(val_ids)) == 0\n",
    "assert len(set(train_ids) & set(test_ids)) == 0\n",
    "assert len(set(val_ids) & set(test_ids)) == 0\n",
    "\n",
    "print(\"Vehicle ID split verified. No leakage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d180f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting MinMaxScaler on training set...\n",
      "Scaling complete.\n",
      "\n",
      "Sample of scaled train_df:\n",
      "   Vehicle_ID  Frame_ID    Global_Time   Local_X   Local_Y     v_Vel  \\\n",
      "0         229      1221  1118847964100  0.016298  0.000000  0.437667   \n",
      "1         229      1231  1118847965100  0.027930  0.012258  0.446000   \n",
      "2         229      1241  1118847966100  0.037263  0.024933  0.454333   \n",
      "3         229      1251  1118847967100  0.051285  0.037638  0.454167   \n",
      "4         229      1261  1118847968100  0.066020  0.050319  0.450667   \n",
      "\n",
      "      v_Acc  Lane_ID  Preceding  Following  delta_x  delta_y  delta_t  \\\n",
      "0  0.500000        1        223          0    0.513   26.264      1.0   \n",
      "1  0.644645        1        223          0    0.506   26.293      1.0   \n",
      "2  0.500000        1        223          0    0.406   27.189      1.0   \n",
      "3  0.500000        1        223          0    0.610   27.254      1.0   \n",
      "4  0.527027        1        223          0    0.641   27.200      1.0   \n",
      "\n",
      "   speed_calc  accel_calc    heading  lane_change  \n",
      "0    0.438044    0.550044  88.881016            0  \n",
      "1    0.438525    0.550636  88.897498            0  \n",
      "2    0.453436    0.568397  89.144494            0  \n",
      "3    0.454583    0.551456  88.717818            0  \n",
      "4    0.453694    0.548950  88.650007            0  \n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 6. Fit MinMaxScaler on TRAIN only\n",
    "# ============================================\n",
    "print(\"\\nFitting MinMaxScaler on training set...\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_df[feature_cols + target_cols])\n",
    "\n",
    "# Apply scaling\n",
    "train_df[feature_cols + target_cols] = scaler.transform(train_df[feature_cols + target_cols])\n",
    "val_df[feature_cols + target_cols]   = scaler.transform(val_df[feature_cols + target_cols])\n",
    "test_df[feature_cols + target_cols]  = scaler.transform(test_df[feature_cols + target_cols])\n",
    "\n",
    "print(\"Scaling complete.\")\n",
    "print(\"\\nSample of scaled train_df:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d576dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved outputs:\n",
      "scaler.pkl\n",
      "train_us101.csv\n",
      "val_us101.csv\n",
      "test_us101.csv\n",
      "\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7. Save scaler + datasets\n",
    "# ============================================\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "train_df.to_csv(\"train_us101.csv\", index=False)\n",
    "val_df.to_csv(\"val_us101.csv\", index=False)\n",
    "test_df.to_csv(\"test_us101.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved outputs:\")\n",
    "print(\"scaler.pkl\")\n",
    "print(\"train_us101.csv\")\n",
    "print(\"val_us101.csv\")\n",
    "print(\"test_us101.csv\")\n",
    "print(\"\\nProcessing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
